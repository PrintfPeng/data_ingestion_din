from __future__ import annotations

import os
import re
import json
import logging
import math
from typing import Dict, List, Optional
from pathlib import Path
from difflib import SequenceMatcher

from dotenv import load_dotenv

# [CHANGE] ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Import ‡πÄ‡∏õ‡πá‡∏ô ChatOpenAI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Custom API
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage
    _HAS_GENAI = True
except Exception:
    ChatOpenAI = None  # type: ignore
    HumanMessage = None  # type: ignore
    SystemMessage = None  # type: ignore
    _HAS_GENAI = False

# --- Re-ranking imports ---
try:
    from sentence_transformers import CrossEncoder
    _HAS_RERANKER = True
    _RERANK_MODEL = None  # Lazy load
except ImportError:
    _HAS_RERANKER = False
    _RERANK_MODEL = None

from .vector_store import search_similar

try:
    from langchain_google_genai import ChatGoogleGenerativeAI
except ImportError:
    ChatGoogleGenerativeAI = None

logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# paths & env
# -------------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parents[2]
INGESTED_DIR = PROJECT_ROOT / "ingested"

_QNA_CACHE: Dict[str, List[Dict[str, str]]] = {}
_QNA_CACHE_MAX_SIZE = 100

# load .env to make sure key available
load_dotenv(override=True)


_CUSTOM_API_KEY = os.getenv("CUSTOM_API_KEY")
_CUSTOM_API_BASE = os.getenv("CUSTOM_API_BASE")

# [CHANGE] ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Model ‡πÄ‡∏õ‡πá‡∏ô Qwen ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
_LL_MODEL_FAST = os.getenv("CUSTOM_MODEL_NAME", "qwen/qwen-2.5-72b-instruct")
# ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô Fallback ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
_LL_MODEL_SMALL = _LL_MODEL_FAST 

_DEFAULT_TEMPERATURE = 0.1 # ‡∏•‡∏î Temperature ‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏°‡∏±‡πà‡∏ß

# Re-ranking config
_RERANK_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"

# [CONFIG] Thresholds ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡∏∂‡πâ‡∏ô
MIN_SCORE_THRESHOLD = 0.25 # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á
MIN_KEYWORD_OVERLAP = 1    # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≥ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏¢‡∏≤‡∏ß)

INTENT_THRESHOLDS = {
    "qna_match": 0.20,
    "table": 0.15,
    "text": 0.10,
    "both": 0.15
}

# Q&A detection regex
_QNA_PATTERN = re.compile(
    r"(?:\d+\s*[\.\-\)]\s*)?"
    r"(?:‡∏ñ‡∏≤‡∏°|q|question)\s*[:\-]?\s*"
    r"(?P<q>.+?)\s*"
    r"(?:‡∏ï‡∏≠‡∏ö|a|answer)\s*[:\-]?\s*"
    r"(?P<a>.+?)(?=(?:\d+\s*[\.\-\)]\s*)?(?:‡∏ñ‡∏≤‡∏°|q|question)\s*[:\-]?|\Z)",
    re.IGNORECASE | re.DOTALL,
)

# Normalize Score Function
def normalize_score(raw_score: float) -> float:
    try:
        return 1 / (1 + math.exp(-raw_score))
    except OverflowError:
        return 0.0 if raw_score < 0 else 1.0


# -------------------------------------------------------------------
# [NEW FIX] Sanitize Document ID (‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Backend)
# -------------------------------------------------------------------
def sanitize_doc_id(doc_id: str) -> str:
    """
    Sanitize document ID to match backend storage format.
    """
    if not doc_id:
        return ""
    # Lowercase
    doc_id = doc_id.lower().strip()
    # Replace spaces with underscores
    doc_id = re.sub(r'\s+', '_', doc_id)
    
    # [CHANGE] ‡πÅ‡∏Å‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏¥‡πà‡∏° \u0E00-\u0E7F (‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏´‡∏±‡∏™‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô
    # ‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°: doc_id = re.sub(r'[^a-z0-9_]', '', doc_id)
    doc_id = re.sub(r'[^a-z0-9_\u0E00-\u0E7F-]', '', doc_id) 
    
    return doc_id


# -------------------------------------------------------------------
# Helper: Sanitization
# -------------------------------------------------------------------
def _sanitize_html_content(html: str) -> str:
    if not html: return ""
    html = re.sub(r"<script.*?>.*?</script>", "", html, flags=re.IGNORECASE | re.DOTALL)
    html = re.sub(r" on\w+=", " data-blocked-event=", html, flags=re.IGNORECASE)
    html = re.sub(r"javascript:", "blocked:", html, flags=re.IGNORECASE)
    return html


# -------------------------------------------------------------------
# Helper: LLM (Custom/OpenAI Compatible) safe getter
# -------------------------------------------------------------------
def _get_llm_instance(model: Optional[str] = None, temperature: float = _DEFAULT_TEMPERATURE):
    if not _HAS_GENAI:
        logger.debug("[rag] langchain_openai not installed -> no LLM available")
        return None
    
    # [CHANGE] ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ Key ‡πÅ‡∏•‡∏∞ Base URL ‡∏à‡∏≤‡∏Å Env ‡πÉ‡∏´‡∏°‡πà
    api_key = os.getenv("CUSTOM_API_KEY") or _CUSTOM_API_KEY
    api_base = os.getenv("CUSTOM_API_BASE") or _CUSTOM_API_BASE
    
    if not api_key:
        logger.debug("[rag] CUSTOM_API_KEY not set -> no LLM available")
        return None

    model = model or _LL_MODEL_FAST
    try:
        # [CHANGE] ‡∏™‡∏£‡πâ‡∏≤‡∏á ChatOpenAI Instance
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            openai_api_key=api_key,
            openai_api_base=api_base,
            max_retries=2,
            request_timeout=60 # ‡πÄ‡∏û‡∏¥‡πà‡∏° timeout ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏ç‡πà‡∏ï‡∏≠‡∏ö‡∏ä‡πâ‡∏≤
            # max_tokens=150 # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
        )
    except Exception as e:
        logger.exception("[rag] Failed to init LLM: %s", e)
        return None


# backend/services/rag.py

def _get_google_llm():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Google Gemini Instance ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key or not ChatGoogleGenerativeAI:
        return None
    
    try:
        return ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", # ‡∏´‡∏£‡∏∑‡∏≠ gemini-1.5-flash
            google_api_key=api_key,
            temperature=0.3,
            max_tokens=2048,
            convert_system_message_to_human=True # ‡∏ö‡∏≤‡∏á‡∏ó‡∏µ Gemini ‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö System msg
        )
    except Exception as e:
        logger.error(f"[rag] Failed to init Google LLM: {e}")
        return None

# -------------------------------------------------------------------
# Helper: Reranker Model (Lazy Load)
# -------------------------------------------------------------------
def _get_reranker_model():
    global _RERANK_MODEL
    if not _HAS_RERANKER:
        return None
    
    if _RERANK_MODEL is None:
        try:
            logger.info(f"[rag] Loading Re-ranking model: {_RERANK_MODEL_NAME}")
            _RERANK_MODEL = CrossEncoder(_RERANK_MODEL_NAME, max_length=512)
        except Exception as e:
            logger.error(f"[rag] Failed to load reranker: {e}")
            return None
    return _RERANK_MODEL


# -------------------------------------------------------------------
# [NEW] Intent & Logic Guards (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ)
# -------------------------------------------------------------------

def _rule_based_intent(query: str) -> Optional[str]:
    # (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô Helper ‡πÅ‡∏ï‡πà logic ‡∏´‡∏•‡∏±‡∏Å‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ auto mode selector)
    if not query or not query.strip(): return None
    q = query.lower()
    table_keywords = ["‡∏ï‡∏≤‡∏£‡∏≤‡∏á", "table", "‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå", "column", "‡πÅ‡∏ñ‡∏ß", "row", "‡∏™‡∏£‡∏∏‡∏õ", "summary", "‡∏¢‡∏≠‡∏î", "amount", "list", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "schedule"]
    image_keywords = ["‡∏£‡∏π‡∏õ", "‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û", "image", "logo", "‡∏Å‡∏£‡∏≤‡∏ü", "graph", "chart", "diagram", "photo", "‡∏†‡∏≤‡∏û"]
    is_table = any(w in q for w in table_keywords)
    is_image = any(w in q for w in image_keywords)
    if is_table and not is_image: return "table"
    if is_image and not is_table: return "both"
    if is_table and is_image: return "both"
    return "text"

def _detect_general_intent(query: str) -> bool:
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
    q = query.lower().strip()
    general_keywords = ["‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "hello", "hi", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ß‡∏±‡∏ô‡∏≠‡∏∞‡πÑ‡∏£", "‡∏≠‡∏≤‡∏Å‡∏≤‡∏®", "who are you", "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏Ñ‡∏£", "‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°"]
    if q in general_keywords:
        return True
    # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ß‡∏•‡∏≤/‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á
    if "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ" in q and "‡∏ß‡∏±‡∏ô‡∏≠‡∏∞‡πÑ‡∏£" in q:
        return True
    return False

def _keyword_overlap_count(query: str, text: str) -> int:
    """‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Query ‡πÅ‡∏•‡∏∞ Chunk Content (Simple Guardrail)"""
    # ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏á‡πà‡∏≤‡∏¢‡πÜ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ PyThaiNLP ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ split space/common chars ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
    q_clean = re.sub(r'[^\w\s]', '', query).lower()
    t_clean = re.sub(r'[^\w\s]', '', text).lower()
    
    q_tokens = set(q_clean.split())
    t_tokens = set(t_clean.split())
    
    # ‡∏ï‡∏±‡∏î Stopwords ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏≠‡∏≠‡∏Å
    stopwords = {"‡∏Ñ‡∏∑‡∏≠", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏≠‡∏¢‡∏π‡πà", "‡∏à‡∏∞", "‡πÑ‡∏î‡πâ", "‡∏ó‡∏µ‡πà", "‡∏ã‡∏∂‡πà‡∏á", "‡∏≠‡∏±‡∏ô", "‡∏Ç‡∏≠‡∏á", "what", "is", "are", "the", "a", "an", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞"}
    q_tokens = q_tokens - stopwords
    
    if not q_tokens: return 0
    return len(q_tokens.intersection(t_tokens))

def _filter_relevant_docs(query: str, docs: list, min_score: float = MIN_SCORE_THRESHOLD) -> list:
    """
    ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏≠‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
    """
    passed = []
    for d in docs:
        score = d.metadata.get("ai_score", 0.0)
        content = d.page_content or ""
        
        # Guard 1: Score Threshold
        if score < min_score:
            continue
            
        # Guard 2: Keyword Overlap (‡∏ñ‡πâ‡∏≤ query ‡∏¢‡∏≤‡∏ß‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£)
        if len(query) > 10: 
            overlap = _keyword_overlap_count(query, content)
            if overlap < MIN_KEYWORD_OVERLAP:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ Keyword ‡∏ï‡∏£‡∏á‡πÄ‡∏•‡∏¢ ‡πÅ‡∏ï‡πà Score ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (Semantic Match) ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô
                if score < 0.75: 
                    continue

        passed.append(d)
    return passed


# -------------------------------------------------------------------
# 3) Build context text
# -------------------------------------------------------------------
def _build_context_text(docs) -> str:
    parts: List[str] = []
    total_tokens = 0
    # MAX_TOKENS_ESTIMATE = 12000
    # [üî• ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ] ‡∏•‡∏î‡∏à‡∏≤‡∏Å 12000 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 4000
    MAX_TOKENS_ESTIMATE = 4000

    parts.append("‚ö†Ô∏è **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:** (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)\n")

    for i, d in enumerate(docs, 1):
        content = getattr(d, "page_content", "") or getattr(d, "content", "") or ""
        content = content.replace("\x00", "") 
        
        if len(content) + total_tokens > MAX_TOKENS_ESTIMATE:
            break

        md = d.metadata or {}
        doc_id = md.get("doc_id", "unknown")
        page = md.get("page", "?")
        score = md.get("ai_score", 0.0)
        
        header = (f"[SOURCE {i}] ID: {doc_id} | Page: {page} | Score: {score:.2f}")
        parts.append(f"{header}\n{content[:3000]}")
        total_tokens += len(content[:3000])

    joined = "\n\n".join(parts)
    return joined


# -------------------------------------------------------------------
# Helper: Generate Fallback Snippets
# -------------------------------------------------------------------
def _generate_fallback_answer(docs, error_msg: str = "") -> str:
    if not docs:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÅ‡∏•‡∏∞ AI ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ)"

    snippets = []
    for i, d in enumerate(docs[:4], 1):
        content = getattr(d, "page_content", "") or getattr(d, "content", "") or ""
        md = d.metadata or {}
        page = md.get('page', '?')
        snippet_text = content[:400].replace("\n", " ").strip() + "..."
        snippets.append(f"**{i}. (‡∏´‡∏ô‡πâ‡∏≤ {page})** {snippet_text}")
    
    joined_snippets = "\n\n".join(snippets)
    header = f"‚ö†Ô∏è **‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô ({error_msg}):** ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∂‡∏á‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏Ñ‡∏£‡∏±‡∏ö:\n\n"
              
    return header + joined_snippets


# -------------------------------------------------------------------
# [NEW] Filter Table Documents by Category/Role
# -------------------------------------------------------------------
def _filter_table_docs_by_category(docs, query: str):
    return docs


# -------------------------------------------------------------------
# [UPDATED] Advanced Re-ranking Logic (Smarter)
# -------------------------------------------------------------------
def _clean_text_for_rerank(text: str) -> str:
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:1000]

def _rerank_documents(query: str, docs: list, top_k: int) -> list:
    if not docs:
        return []

    # 1. Keyword Boosting
    query_terms = query.lower().split()
    scored_docs = []
    for d in docs:
        content = (getattr(d, "page_content", "") or "").lower()
        base_score = 0.0
        
        for term in query_terms:
            if term in content:
                base_score += 1.0
        
        if query.lower() in content:
            base_score += 3.0
            
        # Init metadata if missing
        if "ai_score" not in d.metadata:
            d.metadata["ai_score"] = 0.0
            
        d.metadata["keyword_score"] = base_score
        scored_docs.append(d)

    # 2. AI Re-ranking (Cross Encoder)
    reranker = _get_reranker_model()
    if reranker:
        try:
            valid_pairs_indices = []
            pairs = []
            
            for i, doc in enumerate(scored_docs):
                clean_content = _clean_text_for_rerank(doc.page_content)
                if clean_content:
                    pairs.append([query, clean_content])
                    valid_pairs_indices.append(i)
            
            if pairs:
                raw_scores = reranker.predict(pairs)
                
                for idx, raw in zip(valid_pairs_indices, raw_scores):
                    norm_score = normalize_score(float(raw))
                    scored_docs[idx].metadata["ai_score"] = norm_score
                    scored_docs[idx].metadata["raw_score"] = float(raw)
                
                # Sort by AI Score
                scored_docs.sort(key=lambda x: x.metadata["ai_score"], reverse=True)
                return scored_docs[:top_k]

        except Exception as e:
            logger.warning(f"[rag] Re-ranking failed: {e}")

    # 3. Sort & Cut (Fallback to keyword score)
    scored_docs.sort(key=lambda x: x.metadata.get("keyword_score", 0), reverse=True)
    # Assign dummy confidence for filter to work
    for d in scored_docs:
        if d.metadata["ai_score"] == 0.0:
            d.metadata["ai_score"] = 0.3 # Dummy score to pass filter if keyword matches
    
    return scored_docs[:top_k]


# -------------------------------------------------------------------
# 4) Q&A extraction + matching utilities
# -------------------------------------------------------------------
def _load_qna_pairs_for_doc(doc_id: str) -> List[Dict[str, str]]:
    if len(_QNA_CACHE) > _QNA_CACHE_MAX_SIZE:
        _QNA_CACHE.clear()

    if doc_id in _QNA_CACHE:
        return _QNA_CACHE[doc_id]

    path = INGESTED_DIR / doc_id / "text.json"
    if not path.exists():
        _QNA_CACHE[doc_id] = []
        return []

    try:
        raw = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        _QNA_CACHE[doc_id] = []
        return []

    full = "\n".join((item.get("content") or "") for item in raw)
    pairs: List[Dict[str, str]] = []
    for m in _QNA_PATTERN.finditer(full):
        q = " ".join(m.group("q").split())
        a = " ".join(m.group("a").split())
        if q and a:
            pairs.append({"question": q, "answer": a})
    _QNA_CACHE[doc_id] = pairs
    return pairs


def _simple_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a, b).ratio()


def _find_best_qna_answer_from_docs(query: str, docs) -> Optional[Dict]:
    qna_doc_ids = sorted({
        (d.metadata or {}).get("doc_id")
        for d in docs
        if (d.metadata or {}).get("doc_id") 
    })
    qna_doc_ids = [d for d in qna_doc_ids if d]
    
    if not qna_doc_ids:
        return None

    all_pairs = []
    for doc_id in qna_doc_ids:
        pairs = _load_qna_pairs_for_doc(doc_id)
        for p in pairs:
            all_pairs.append({"question": p["question"], "answer": p["answer"], "doc_id": doc_id})

    if not all_pairs:
        return None

    best_score = 0.0
    best_item = None
    
    reranker = _get_reranker_model()
    if reranker:
        try:
            input_pairs = [[query, p["question"]] for p in all_pairs]
            raw_scores = reranker.predict(input_pairs)
            
            for i, raw in enumerate(raw_scores):
                norm_score = normalize_score(float(raw))
                if norm_score > best_score:
                    best_score = norm_score
                    best_item = all_pairs[i]
        except Exception:
            pass

    if not best_item:
        for p in all_pairs:
            score = _simple_similarity(query, p["question"])
            if score > best_score:
                best_score = score
                best_item = p
        
    if best_item and best_score >= 0.75: # High confidence only
        return {
            "answer": best_item["answer"],
            "sources": [{"doc_id": best_item["doc_id"], "source": "Q&A Match", "page": "?"}],
            "score": float(best_score)
        }
    return None


# -------------------------------------------------------------------
# 5) main RAG function (UPGRADED & ROBUST)
# -------------------------------------------------------------------
async def answer_question(
    query: str,
    doc_ids: Optional[List[str]] = None,
    top_k: int = 10,
    mode: str = "auto",
) -> Dict:
    
    # 1. Input Check
    if not query or not query.strip():
        return {"answer": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏£‡∏±‡∏ö", "sources": [], "intent": None, "mode": mode}

    # [NEW] STEP 1: General Intent Guard
    if _detect_general_intent(query):
        return {
            "answer": "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏ú‡∏°‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö (‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö)",
            "sources": [],
            "intent": "general",
            "mode": mode
        }

    # [NEW] STEP 2: Mode Selection (Deterministic)
    if mode == "auto":
        q_lower = query.lower()
        if any(x in q_lower for x in ["‡∏ï‡∏≤‡∏£‡∏≤‡∏á", "table", "‡∏¢‡∏≠‡∏î", "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥", "list", "‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£", "‡∏™‡∏£‡∏∏‡∏õ"]):
            intent = "table"
        else:
            intent = "text"
    else:
        intent = mode

    # Sanitize doc_ids
    sanitized_doc_ids = None
    if doc_ids:
        sanitized_doc_ids = [sanitize_doc_id(doc_id) for doc_id in doc_ids if doc_id]

    doc_types = None
    sources_filter = None 

    # 3. Search (3-Layer Fallback Strategy)
    docs = []
    raw_docs = []

    try:
        # Layer 1: Strict Search
        raw_docs = search_similar(query, k=top_k*3, doc_ids=sanitized_doc_ids, sources=sources_filter, doc_types=doc_types)
        
        # [CHANGE] Disabled Layer 2 & 3 to prevent cross-document contamination
        # ‡∏ñ‡πâ‡∏≤ Layer 1 (Strict) ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•" ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏°‡∏±‡πà‡∏ß)
        
        # Layer 2: Relaxed ID Search (DISABLED)
        # if not raw_docs and sanitized_doc_ids:
        #      logger.warning(f"[rag] Layer 1 empty. Retrying Layer 2 (Global search).")
        #      raw_docs = search_similar(query, k=top_k*3, doc_ids=None, sources=sources_filter, doc_types=doc_types)
        
        # Layer 3: Keyword/Broad Search (DISABLED)
        # if not raw_docs:
        #      logger.warning(f"[rag] Layer 2 empty. Retrying Layer 3 (Broad).")
        #      broad_docs = search_similar(query, k=50, doc_ids=None, sources=sources_filter, doc_types=doc_types)
        #      
        #      q_terms = [t for t in query.lower().split() if len(t) > 2]
        #      if q_terms:
        #          raw_docs = [d for d in broad_docs if any(t in (d.page_content or "").lower() for t in q_terms)]
        #          if not raw_docs: raw_docs = broad_docs[:top_k]
        #      else:
        #          raw_docs = broad_docs[:top_k]

        logger.info(f"[rag] Found {len(raw_docs)} raw docs")

        # Rerank
        docs = _rerank_documents(query, raw_docs, top_k)
        
        # [NEW] STEP 4: STRICT FILTERING (No Rescue Mission)
        relevant_docs = _filter_relevant_docs(query, docs, min_score=MIN_SCORE_THRESHOLD)
        
        if not relevant_docs:
            # Check Q&A direct match before giving up
            qna_match = _find_best_qna_answer_from_docs(query, docs) # Use original docs to find doc_id context
            if qna_match:
                return {
                    "answer": qna_match["answer"],
                    "sources": qna_match["sources"],
                    "intent": "qna",
                    "mode": f"{mode}+qna"
                }
            
            return {
                "answer": "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö‡∏°‡∏≤‡∏Ñ‡∏£‡∏±‡∏ö (Relevance Score ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)",
                "sources": [],
                "intent": intent,
                "mode": mode
            }
            
        docs = relevant_docs # Use filtered docs

    except Exception as e:
        logger.error(f"[rag] Search failed: {e}")
        return {"answer": f"‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á: {str(e)}", "sources": [], "intent": intent, "mode": mode}

# --- Prepare Context & Table Map (FIXED) ---
    table_map = {}
    table_cat_map = {}
    context_parts = [] # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô Context ‡πÉ‡∏´‡∏ç‡πà
    table_counter = 0 
    
    # [NEW] ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ ID ‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Search Result ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏ì‡∏µ AI ‡πÑ‡∏°‡πà‡∏¢‡∏≠‡∏°‡∏ï‡∏≠‡∏ö (Fail-safe)
    found_table_ids = []
    
    try:
        context_parts.append("‚ö†Ô∏è **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:** (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á)\n")

        for i, d in enumerate(docs, 1):
            md = d.metadata or {}
            doc_id = md.get("doc_id", "unknown")
            page = md.get("page", "?")
            source = str(md.get("source", "text")).lower().strip()
            # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ (Markdown) ‡∏°‡∏≤‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ
            content = getattr(d, "page_content", "") or ""
            content = content.replace("\x00", "")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á Chunk ‡∏ô‡∏µ‡πâ
            chunk_header = f"[SOURCE {i}] ID: {doc_id} | Page: {page}"

            if source == "table":
                table_counter += 1
                table_ref_id = str(table_counter) # ‡πÄ‡∏•‡∏Ç‡∏£‡∏±‡∏ô 1, 2, 3...
                
                # ‡πÄ‡∏Å‡πá‡∏ö ID ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Fail-safe
                found_table_ids.append(table_ref_id)
                
                # 1. ‡∏î‡∏∂‡∏á HTML ‡∏°‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Map (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö)
                raw_html = md.get("html_content", "")
                safe_html = _sanitize_html_content(raw_html)
                if not safe_html:
                    # Fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ HTML ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á Markdown ‡πÉ‡∏ô‡∏Å‡∏•‡πà‡∏≠‡∏á‡πÅ‡∏ó‡∏ô
                    safe_html = f"<pre class='text-xs overflow-auto p-2 bg-gray-100'>{md.get('markdown_content', 'No content')}</pre>"
                
                table_map[table_ref_id] = safe_html
                
                # 2. [CRITICAL FIX] ‡πÅ‡∏õ‡∏∞‡∏õ‡πâ‡∏≤‡∏¢‡∏ö‡∏≠‡∏Å AI ‡∏ä‡∏±‡∏î‡πÜ ‡∏ß‡πà‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏´‡∏±‡∏™‡∏≠‡∏∞‡πÑ‡∏£
                # AI ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ Markdown ‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∑‡∏≠ TBL_1
                chunk_header += f" | **TYPE: TABLE (Code: [SHOW_TABLE:TBL_{table_ref_id}])**"
                
                # Mapping category/role (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
                category = md.get("category", "").strip().lower()
                role = md.get("role", "").strip().lower()
                
                if category:
                    cat_key = f"cat:{category}"
                    if cat_key not in table_cat_map: table_cat_map[cat_key] = safe_html
                if role:
                    role_key = f"role:{role}"
                    if role_key not in table_cat_map: table_cat_map[role_key] = safe_html
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏•‡∏á‡πÉ‡∏ô Context Parts
            context_parts.append(f"{chunk_header}\n{content[:3500]}")

        # ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI
        context_text = "\n\n".join(context_parts)

    except Exception as e:
        logger.error(f"[rag] Context build failed: {e}")
        return {"answer": "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", "sources": [], "intent": intent, "mode": mode}
    
    # [NEW] Strict System Prompt for Table Mode
    if mode == "table":
        # Prompt ‡πÇ‡∏´‡∏î: ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡πà‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏π‡∏î‡πÄ‡∏¢‡∏≠‡∏∞
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á '‡∏ï‡∏≤‡∏£‡∏≤‡∏á' ‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\n"
            "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:\n"
            "1. ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô CONTEXT ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á (SOURCE: Table) ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n"
            "2. ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏´‡∏±‡∏™ [SHOW_TABLE:TBL_x] ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏´‡πâ‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°\n"
            "3. ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö ‡πÄ‡∏ä‡πà‡∏ô [SHOW_TABLE:TBL_1] [SHOW_TABLE:TBL_2]\n"
            "4. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠\n"
            "\n"
            f"=== CONTEXT ===\n{context_text}\n==============="
        )
    else:
        # Prompt ‡∏õ‡∏Å‡∏ï‡∏¥
        system_prompt = (
            "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Smart Assistant) ‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ (Context) ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n"
            "‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:\n"
            "1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** ‡∏à‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏£‡∏≤‡∏ö‡∏≠‡∏∞‡πÑ‡∏£\n"
            "2. **‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Context ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á\n"
            "   - ‚ö†Ô∏è **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Context ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡πÜ ‡∏Å‡πà‡∏≠‡∏ô\n"
            "3. **‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:** ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏•‡∏∞‡∏™‡∏•‡∏ß‡∏¢\n"
            "4. **‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô):**\n"
            "   - [SHOW_TABLE:TBL_x] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç (‡πÄ‡∏ä‡πà‡∏ô TBL_1)\n"
            "   - [SHOW_TABLE:CAT=‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà] ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏°‡∏ß‡∏î\n"
            "5. **[‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç] ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á:**\n"
            "   - ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Table (SOURCE Type: Table) **‡∏ï‡πâ‡∏≠‡∏á** ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏≤‡∏ï‡∏≠‡∏ö ‡∏≠‡∏¢‡πà‡∏≤‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n"
            "   - ‡∏ñ‡πâ‡∏≤‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ü‡∏±‡∏ô‡∏ò‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡∏¢\n"
            "\n"
            "‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:\n"
            "- ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏ô‡∏≠‡∏Å‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô Context\n"
            "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏ö‡∏°‡∏≤'\n"
            "- ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏≠‡∏á (Anti-Hallucination)\n"
            "\n"
            "[IMAGE HANDLING]\n"
            "‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ô Context ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n"
            "1. ‡∏°‡∏≠‡∏á‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 'Path: ...' ‡πÉ‡∏ô Context\n"
            "2. ‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Tag ‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏£‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: [SHOW_IMAGE: <path_file>]\n"
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: '‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö [SHOW_IMAGE: ingested/doc_001/images/img_1.png]'"
            f"=== CONTEXT ===\n{context_text}\n==============="
        )
# -------------------------------------------------------------------
    # 4) Call LLM (Chain of Fallback: OpenRouter -> Google -> Raw)
    # -------------------------------------------------------------------
    llm = _get_llm_instance(model=_LL_MODEL_FAST)
    
    answer_text = ""
    ai_response = None
    
    # --- 1. ‡πÅ‡∏ú‡∏ô A: ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Primary LLM (OpenRouter/Qwen) ---
    try:
        if llm:
            # logger.info(f"[rag] üöÄ Trying Primary LLM ({_LL_MODEL_FAST})...")
            ai_response = await llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=query)])
            answer_text = getattr(ai_response, "content", str(ai_response))
    except Exception as e:
        logger.warning(f"[rag] ‚ùå Primary LLM failed: {e}")

    # --- 2. ‡πÅ‡∏ú‡∏ô B: ‡∏ñ‡πâ‡∏≤‡πÅ‡∏ú‡∏ô A ‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Google Gemini (Backup) ---
    if not answer_text or answer_text == "AI Error":
        try:
            google_llm = _get_google_llm() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ
            if google_llm:
                logger.info("[rag] üîÑ Switching to Backup LLM: Google Gemini...")
                # ‡πÉ‡∏ä‡πâ ainvoke ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö Async ‡πÑ‡∏°‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
                ai_response = await google_llm.ainvoke([SystemMessage(content=system_prompt), HumanMessage(content=query)])
                answer_text = getattr(ai_response, "content", str(ai_response))
            else:
                logger.warning("[rag] Google API Key not found, skipping backup.")
        except Exception as e_google:
             logger.error(f"[rag] ‚ùå Google LLM also failed: {e_google}")

    # --- 3. ‡πÅ‡∏ú‡∏ô C (‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢): ‡∏ñ‡πâ‡∏≤ Google ‡∏Å‡πá‡∏û‡∏±‡∏á‡∏≠‡∏µ‡∏Å (‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏°‡∏î Table ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö) ---
    # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡∏´‡∏£‡∏∑‡∏≠ ‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    if not answer_text:
        
        # [Fail-safe] ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î Table ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏≤‡πÄ‡∏à‡∏≠ ID ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô Search (found_table_ids ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤)
        # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á "" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Code Override ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (Section 5) ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡∏∂‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏°‡∏≤‡πÇ‡∏ä‡∏ß‡πå‡πÄ‡∏≠‡∏á
        if mode == "table" and found_table_ids:
             answer_text = "" 
        else:
             # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÉ‡∏´‡πâ‡∏¢‡∏≠‡∏°‡πÅ‡∏û‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏™‡∏î‡∏á Raw Fallback (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö)
             logger.warning("[rag] ‚ö†Ô∏è All LLMs failed. Using Raw Fallback.")
             return {
                 "answer": _generate_fallback_answer(docs, "System Error"), 
                 "sources": [], 
                 "intent": intent, 
                 "mode": f"{mode}+error"
             }

    # --- 5) Regex Replacement ---
    if (table_map or table_cat_map) and answer_text:
        try:
            pattern_cat = re.compile(r"\[(?:SHOW_TABLE|SHOW|TABLE)[^:]*:\s*CAT\s*=\s*([^\]]+)\]", re.IGNORECASE)
            
            def replace_cat(match):
                cat_name = match.group(1).strip().lower()
                cat_key = f"cat:{cat_name}"
                if cat_key in table_cat_map: return f"\n<div class='my-4 overflow-x-auto border rounded-lg shadow-sm bg-white p-2'>{table_cat_map[cat_key]}</div>\n"
                role_key = f"role:{cat_name}"
                if role_key in table_cat_map: return f"\n<div class='my-4 overflow-x-auto border rounded-lg shadow-sm bg-white p-2'>{table_cat_map[role_key]}</div>\n"
                return match.group(0)
            
            answer_text = pattern_cat.sub(replace_cat, answer_text)
            
            def replace_match(match):
                found_id = match.group(1)
                # Handle TBL_1 format vs 1
                clean_id = found_id.replace("TBL_", "").strip()
                
                if clean_id in table_map: return f"\n<div class='my-4 overflow-x-auto border rounded-lg shadow-sm bg-white p-2'>{table_map[clean_id]}</div>\n"
                
                if "." in found_id:
                    simple_id = found_id.split(".")[0]
                    if simple_id in table_map: return f"\n<div class='my-4 overflow-x-auto border rounded-lg shadow-sm bg-white p-2'>{table_map[simple_id]}</div>\n"
                if len(table_map) == 1:
                    first_key = list(table_map.keys())[0]
                    return f"\n<div class='my-4 overflow-x-auto border rounded-lg shadow-sm bg-white p-2'>{table_map[first_key]}</div>\n"
                return match.group(0)

            pattern = re.compile(r"\[(?:SHOW_TABLE|SHOW|TABLE)[^:]*:\s*(?:TBL[_]?)?\s*([\d\.]+)\]", re.IGNORECASE)
            answer_text = pattern.sub(replace_match, answer_text)
            
        except Exception as e:
            logger.error(f"[rag] Regex replacement failed: {e}")

    # 6) Sources
    sources = []
    for d in docs:
        md = d.metadata or {}
        sources.append({
            "doc_id": md.get("doc_id"),
            "page": md.get("page"),
            "source": md.get("source"),
            "chunk_id": md.get("chunk_id")
        })

    return {"answer": answer_text, "sources": sources, "intent": intent, "mode": f"{mode}+qna_llm"}