from __future__ import annotations

from pathlib import Path
from typing import List, Optional, Dict, Tuple
import logging
import warnings
import re
import gc  # [FIX 1] ‡πÄ‡∏û‡∏¥‡πà‡∏° import gc ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Memory/File Lock

from fastapi import HTTPException
from langchain_community.vectorstores import Chroma
# [CHANGE] ‡∏•‡∏ö Google specific error import ‡∏≠‡∏≠‡∏Å
# from langchain_google_genai._common import GoogleGenerativeAIError
from langchain_core.documents import Document

from .chunking import Chunk
from .embeddings import get_embedding_client


# -----------------------------------------------------------
# Setup
# -----------------------------------------------------------
logger = logging.getLogger(__name__)

# Fallback error import
try:
    from chromadb.errors import InternalError as ChromaInternalError
except Exception:
    ChromaInternalError = Exception

# -----------------------------------------------------------
# Configuration
# -----------------------------------------------------------

CHROMA_DIR = "chroma_db"
COLLECTION_NAME = "documents"

# Cache vectordb ‡∏ï‡∏≤‡∏° (persist_directory, collection_name)
_vectordb_cache: Dict[Tuple[str, str], Chroma] = {}


# -----------------------------------------------------------
# [NEW] Sanitize Document ID (‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö rag.py)
# -----------------------------------------------------------
def sanitize_doc_id(doc_id: str) -> str:
    """
    Sanitize document ID to match backend storage format.
    """
    if not doc_id:
        return ""
    # Lowercase
    doc_id = doc_id.lower().strip()
    # Replace spaces with underscores
    doc_id = re.sub(r'\s+', '_', doc_id)
    
    # [CHANGE] ‡πÅ‡∏Å‡πâ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ: ‡πÄ‡∏û‡∏¥‡πà‡∏° \u0E00-\u0E7F (‡∏ä‡πà‡∏ß‡∏á‡∏£‡∏´‡∏±‡∏™‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‡∏•‡∏á‡πÑ‡∏õ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô
    # ‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°: doc_id = re.sub(r'[^a-z0-9_]', '', doc_id)
    doc_id = re.sub(r'[^a-z0-9_\u0E00-\u0E7F-]', '', doc_id) 
    
    return doc_id


# -----------------------------------------------------------
# Helper: Chroma DB Management
# -----------------------------------------------------------

def _cache_key(persist_directory: str, collection_name: str) -> Tuple[str, str]:
    return (str(Path(persist_directory).resolve()), collection_name)


def get_vector_store(
    persist_directory: str = CHROMA_DIR,
    collection_name: str = COLLECTION_NAME,
    force_recreate: bool = False,
    reload: bool = False, # [FIX] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Reload
) -> Chroma:
    """
    ‡∏Ñ‡∏∑‡∏ô Chroma vector store ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡∏Å‡∏Å‡∏±‡∏ö Embeddings client (‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô Custom API ‡πÅ‡∏•‡πâ‡∏ß)
    - ‡∏°‡∏µ cache ‡πÉ‡∏ô process ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    """
    should_reload = force_recreate or reload # ‡∏£‡∏ß‡∏° Flag

    persist_path = Path(persist_directory)
    persist_path.mkdir(parents=True, exist_ok=True)
    key = _cache_key(persist_directory, collection_name)

    # [FIX] ‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πà‡∏á Reload ‡πÉ‡∏´‡πâ‡∏•‡∏ö Cache ‡∏ó‡∏¥‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡πà‡∏á GC ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏•‡∏î File Lock
    if should_reload and key in _vectordb_cache:
        logger.info(f"[vector_store] Forcing reload of ChromaDB client for {key}")
        del _vectordb_cache[key]
        gc.collect() # [FIX 2] ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏•‡πâ‡∏≤‡∏á Memory ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ File Lock ‡∏ö‡∏ô Windows

    if not force_recreate and key in _vectordb_cache:
        return _vectordb_cache[key]

    embeddings = get_embedding_client()

    try:
        # Suppress deprecation warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            vectordb = Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=str(persist_path),
            )
    except Exception as e:
        logger.exception("[vector_store] Failed to init Chroma: %s", e)
        # ‡∏•‡∏≠‡∏á GC ‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß Retry ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡∏ä‡∏ô‡∏Å‡∏±‡∏ô
        gc.collect()
        try:
            vectordb = Chroma(
                collection_name=collection_name,
                embedding_function=embeddings,
                persist_directory=str(persist_path),
            )
        except Exception:
            raise HTTPException(
                status_code=500,
                detail="‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Vector DB ‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á"
            ) from e

    _vectordb_cache[key] = vectordb
    return vectordb

# -----------------------------------------------------------------------------
# [NEW] ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏á Cache ‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πà‡∏á‡∏ï‡∏≤‡∏¢ (Global Reset)
# -----------------------------------------------------------------------------
def reset_vector_store_cache():
    """
    ‡∏ó‡πà‡∏≤‡πÑ‡∏°‡πâ‡∏ï‡∏≤‡∏¢: ‡∏™‡∏±‡πà‡∏á‡∏•‡πâ‡∏≤‡∏á Cache ‡∏Ç‡∏≠‡∏á Vector DB ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
    ‡πÉ‡∏ä‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏≠‡∏ô Upload ‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î DB ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
    """
    global _vectordb_cache
    
    if _vectordb_cache:
        print(f"[vector_store] üßπ Force clearing {_vectordb_cache} cache entries...")
        _vectordb_cache.clear()
    
    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö Python ‡∏Ñ‡∏∑‡∏ô RAM ‡πÅ‡∏•‡∏∞‡∏õ‡∏•‡∏î File Lock ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Windows Error)
    try:
        import gc
        gc.collect()
        print("[vector_store] üóëÔ∏è Garbage collection done.")
    except Exception as e:
        print(f"[vector_store] GC Error: {e}")

def _normalize_metadata(md: dict) -> dict:
    """‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ complex types ‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Chroma ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏î‡πâ"""
    simple: dict = {}
    for k, v in (md or {}).items():
        if isinstance(v, (str, int, float, bool)) or v is None:
            simple[k] = v
        else:
            try:
                simple[k] = str(v)
            except Exception:
                simple[k] = repr(v)
    return simple

# -----------------------------------------------------------
# 1) Indexing: ‡πÄ‡∏≠‡∏≤ chunks ‡πÑ‡∏õ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Chroma
# -----------------------------------------------------------

def index_chunks(
    chunks: List[Chunk],
    persist_directory: str = CHROMA_DIR,
    collection_name: str = COLLECTION_NAME,
) -> None:
    if not chunks:
        return

    vectordb = get_vector_store(persist_directory, collection_name)
    texts = [c.content for c in chunks]
    
    # [FIX] Sanitize doc_id ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏Å‡πá‡∏ö
    raw_metadatas = [
        (c.metadata or {}) | {
            "doc_id": sanitize_doc_id(c.doc_id),  # [CRITICAL FIX]
            "doc_type": c.doc_type,
            "source": c.source,
            "page": c.page,
            "chunk_id": c.id,
        } for c in chunks
    ]
    
    metadatas = [_normalize_metadata(md) for md in raw_metadatas]
    ids = [c.id for c in chunks]

    try:
        logger.info(f"[vector_store] Indexing {len(chunks)} chunks...")
        vectordb.add_texts(texts=texts, metadatas=metadatas, ids=ids)
        
        # [DEBUG] ‡πÅ‡∏™‡∏î‡∏á doc_id ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á‡πÑ‡∏õ
        unique_doc_ids = set(md["doc_id"] for md in raw_metadatas)
        logger.info(f"[vector_store] Indexed doc_ids: {unique_doc_ids}")
        
        try:
            vectordb.persist()
        except Exception: 
            pass
    # [CHANGE] ‡∏•‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö GoogleGenerativeAIError ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö error ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠ OpenAI error
    except Exception as e:
        logger.exception("[vector_store] Indexing error: %s", e)
        raise HTTPException(status_code=500, detail=f"Indexing error: {e}") from e


# -----------------------------------------------------------
# 2) Search: Pure Retrieval (COMPLETELY REWRITTEN)
# -----------------------------------------------------------

def _python_filter_documents(
    raw_docs: List[Document], 
    doc_ids: Optional[List[str]], 
    sources: Optional[List[str]], 
    doc_types: Optional[List[str]]
) -> List[Document]:
    """
    [IMPROVED] ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏û‡∏£‡πâ‡∏≠‡∏° Sanitization
    """
    filtered = []
    
    # [FIX] Sanitize doc_ids ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏á
    sanitized_doc_ids = None
    if doc_ids:
        sanitized_doc_ids = set(sanitize_doc_id(d) for d in doc_ids)
    
    for d in raw_docs:
        md = d.metadata or {}
        
        # [DEBUG] Log metadata ‡∏Ç‡∏≠‡∏á document ‡πÅ‡∏£‡∏Å
        if not filtered:
            logger.debug(f"[vector_store] Sample metadata: {md}")
        
        # Check doc_ids (WITH SANITIZATION)
        if sanitized_doc_ids:
            found_id = md.get("doc_id")
            if not found_id:
                continue
            
            # [FIX] Sanitize ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö
            normalized_found_id = sanitize_doc_id(str(found_id))
            if normalized_found_id not in sanitized_doc_ids:
                continue
                
        # Check sources
        if sources:
            doc_source = md.get("source")
            if not doc_source or str(doc_source) not in sources:
                continue
            
        # Check doc_types
        if doc_types:
            doc_type = md.get("doc_type")
            if not doc_type or str(doc_type) not in doc_types:
                continue
            
        filtered.append(d)
    
    return filtered


def search_similar(
    query: str,
    k: int = 5,
    persist_directory: str = CHROMA_DIR,
    collection_name: str = COLLECTION_NAME,
    doc_ids: Optional[List[str]] = None,
    sources: Optional[List[str]] = None,
    doc_types: Optional[List[str]] = None,
) -> List[Document]:
    """
    [COMPLETELY REWRITTEN] ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö Robust with Smart Fallback
    """
    if not query or not query.strip():
        raise HTTPException(status_code=400, detail="Empty query")

    vectordb = get_vector_store(persist_directory, collection_name)

    # [FIX] Sanitize doc_ids ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á filter
    sanitized_doc_ids = None
    if doc_ids:
        sanitized_doc_ids = [sanitize_doc_id(d) for d in doc_ids if d]
        logger.info(f"[vector_store] Original doc_ids: {doc_ids} -> Sanitized: {sanitized_doc_ids}")

    # --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á (SIMPLIFIED) ---
    where_filter = {}
    
    # [FIX] ‡πÉ‡∏ä‡πâ Simple Equality ‡πÅ‡∏ó‡∏ô $in ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Single Value
    if sanitized_doc_ids:
        if len(sanitized_doc_ids) == 1:
            where_filter["doc_id"] = sanitized_doc_ids[0]
        else:
            # [FIX] ‡∏ö‡∏≤‡∏á Chroma version ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö $in ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á OR ‡πÅ‡∏ó‡∏ô
            # ‡πÅ‡∏ï‡πà OR ‡∏Å‡πá‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ multiple IDs ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Python Filter ‡πÅ‡∏ó‡∏ô
            where_filter = None  # Force Python Filter
            
    if sources:
        if len(sources) == 1:
            if where_filter is not None:
                where_filter["source"] = sources[0]
        else:
            where_filter = None  # Force Python Filter
            
    if doc_types:
        if len(doc_types) == 1:
            if where_filter is not None:
                where_filter["doc_type"] = doc_types[0]
        else:
            where_filter = None

    # --- 2. Smart Search Strategy ---
    try:
        # [NEW] Strategy: ‡∏ñ‡πâ‡∏≤ Filter ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô (multiple values) ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≤‡∏° Native Filter ‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
        use_native_filter = where_filter is not None and where_filter != {}
        
        if use_native_filter:
            logger.info(f"[vector_store] Using NATIVE filter: {where_filter}")
            results = vectordb.similarity_search(query, k=k, filter=where_filter)
            
            # Check if native filter worked
            if not results:
                logger.warning(f"[vector_store] Native filter returned 0 results. Switching to Python filter.")
                use_native_filter = False  # Trigger fallback
        
        # Fallback to Python Filter
        if not use_native_filter:
            logger.info(f"[vector_store] Using PYTHON filter for: doc_ids={sanitized_doc_ids}, sources={sources}, doc_types={doc_types}")
            
            # [FIX] ‡∏î‡∏∂‡∏á‡∏°‡∏≤ k*10 ‡πÅ‡∏ó‡∏ô k*5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏à‡∏≠
            fetch_size = max(k * 10, 50)  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 50 ‡∏ï‡∏±‡∏ß
            raw_docs = vectordb.similarity_search(query, k=fetch_size)
            
            logger.info(f"[vector_store] Fetched {len(raw_docs)} raw documents for Python filtering")
            
            # [DEBUG] ‡πÅ‡∏™‡∏î‡∏á doc_ids ‡∏ó‡∏µ‡πà‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÑ‡∏î‡πâ
            if raw_docs:
                found_doc_ids = set(d.metadata.get("doc_id") for d in raw_docs if d.metadata)
                logger.info(f"[vector_store] Available doc_ids in fetched results: {found_doc_ids}")
            
            results = _python_filter_documents(raw_docs, doc_ids, sources, doc_types)[:k]
        
        logger.info(f"[vector_store] Search query='{query[:50]}...' returned {len(results)} results")
        
        return results

    except Exception as e:
        # [CRITICAL FIX] ‡∏î‡∏±‡∏Å‡∏à‡∏±‡∏ö Error Database ‡∏û‡∏±‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏±‡πà‡∏á Reload ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (Auto-healing)
        error_msg = str(e)
        logger.warning(f"[vector_store] Search Exception: {error_msg}")

        is_db_corruption = (
            "Nothing found on disk" in error_msg 
            or "InternalError" in error_msg 
            or "segment reader" in error_msg
            or "sqlite" in error_msg.lower()
            or "Error finding id" in error_msg # [FIX 3] ‡∏î‡∏±‡∏Å Error finding id
        )

        if is_db_corruption or isinstance(e, ChromaInternalError):
            logger.warning("[vector_store] üö® DB Corruption/Change detected. Reloading Vector Store...")
            
            # 1. Force Reload (‡∏•‡∏ö Cache ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà)
            vectordb = get_vector_store(persist_directory, collection_name, reload=True)
            
            # 2. Retry Search with the NEW vectordb instance
            try:
                # ‡πÉ‡∏ä‡πâ Python Filter ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
                raw_docs = vectordb.similarity_search(query, k=k*10)
                results = _python_filter_documents(raw_docs, doc_ids, sources, doc_types)[:k]
                logger.info(f"[vector_store] Retry success. Found {len(results)} results.")
                return results
            except Exception as final_e:
                logger.error(f"[vector_store] Retry failed: {final_e}")
                # return empty list instead of crashing
                return []
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà DB error ‡πÉ‡∏´‡πâ raise ‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥
        raise e


# -----------------------------------------------------------
# [NEW] Debug Helper: Inspect Collection
# -----------------------------------------------------------
def get_collection_info(
    persist_directory: str = CHROMA_DIR,
    collection_name: str = COLLECTION_NAME,
) -> Dict:
    """
    [NEW] Debug function: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Collection
    """
    try:
        # ‡πÉ‡∏ä‡πâ reload=True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏™‡∏°‡∏≠
        vectordb = get_vector_store(persist_directory, collection_name, reload=True)
        
        sample_docs = vectordb.similarity_search("test", k=5)
        
        doc_ids = set()
        sources = set()
        doc_types = set()
        
        for doc in sample_docs:
            md = doc.metadata or {}
            if md.get("doc_id"):
                doc_ids.add(md.get("doc_id"))
            if md.get("source"):
                sources.add(md.get("source"))
            if md.get("doc_type"):
                doc_types.add(md.get("doc_type"))
        
        return {
            "collection_name": collection_name,
            "sample_count": len(sample_docs),
            "unique_doc_ids": list(doc_ids),
            "unique_sources": list(sources),
            "unique_doc_types": list(doc_types),
            "sample_metadata": [doc.metadata for doc in sample_docs[:3]]
        }
    except Exception as e:
        logger.exception("[vector_store] Failed to get collection info: %s", e)
        return {"error": str(e)}